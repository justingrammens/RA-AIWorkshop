{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Evaluation of RAG Systems using deepeval\n",
    "\n",
    "## Overview\n",
    "\n",
    "This code demonstrates the use of the `deepeval` library to perform comprehensive evaluations of Retrieval-Augmented Generation (RAG) systems. It covers various evaluation metrics and provides a framework for creating and running test cases.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. Correctness Evaluation\n",
    "2. Faithfulness Evaluation\n",
    "3. Contextual Relevancy Evaluation\n",
    "4. Combined Evaluation of Multiple Metrics\n",
    "5. Batch Test Case Creation\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "### 1. Correctness (GEval)\n",
    "\n",
    "- Evaluates whether the actual output is factually correct based on the expected output.\n",
    "- Uses GPT-4 as the evaluation model.\n",
    "- Compares the expected and actual outputs.\n",
    "\n",
    "### 2. Faithfulness (FaithfulnessMetric)\n",
    "\n",
    "- Assesses whether the generated answer is faithful to the provided context.\n",
    "- Uses GPT-4 as the evaluation model.\n",
    "- Can provide detailed reasons for the evaluation.\n",
    "\n",
    "### 3. Contextual Relevancy (ContextualRelevancyMetric)\n",
    "\n",
    "- Evaluates how relevant the retrieved context is to the question and answer.\n",
    "- Uses GPT-4 as the evaluation model.\n",
    "- Can provide detailed reasons for the evaluation.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. Flexible Metric Configuration: Each metric can be customized with different models and parameters.\n",
    "2. Multi-Metric Evaluation: Ability to evaluate test cases using multiple metrics simultaneously.\n",
    "3. Batch Test Case Creation: Utility function to create multiple test cases efficiently.\n",
    "4. Detailed Feedback: Options to include detailed reasons for evaluation results.\n",
    "\n",
    "## Benefits of this Approach\n",
    "\n",
    "1. Comprehensive Evaluation: Covers multiple aspects of RAG system performance.\n",
    "2. Flexibility: Easy to add or modify evaluation metrics and test cases.\n",
    "3. Scalability: Capable of handling multiple test cases and metrics efficiently.\n",
    "4. Interpretability: Provides detailed reasons for evaluation results, aiding in system improvement.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This deep evaluation approach using the `deepeval` library offers a robust framework for assessing the performance of RAG systems. By evaluating correctness, faithfulness, and contextual relevancy, it provides a multi-faceted view of system performance. This comprehensive evaluation is crucial for identifying areas of improvement and ensuring the reliability and effectiveness of RAG systems in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepeval in ./.venv/lib/python3.13/site-packages (3.0.3)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.13/site-packages (from deepeval) (3.12.6)\n",
      "Requirement already satisfied: anthropic<0.50.0,>=0.49.0 in ./.venv/lib/python3.13/site-packages (from deepeval) (0.49.0)\n",
      "Requirement already satisfied: click<8.2.0,>=8.0.0 in ./.venv/lib/python3.13/site-packages (from deepeval) (8.1.8)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.9.0 in ./.venv/lib/python3.13/site-packages (from deepeval) (1.19.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.67.1 in ./.venv/lib/python3.13/site-packages (from deepeval) (1.72.1)\n",
      "Requirement already satisfied: nest_asyncio in ./.venv/lib/python3.13/site-packages (from deepeval) (1.6.0)\n",
      "Requirement already satisfied: ollama in ./.venv/lib/python3.13/site-packages (from deepeval) (0.5.1)\n",
      "Requirement already satisfied: openai in ./.venv/lib/python3.13/site-packages (from deepeval) (1.82.1)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.24.0 in ./.venv/lib/python3.13/site-packages (from deepeval) (1.34.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 in ./.venv/lib/python3.13/site-packages (from deepeval) (1.34.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in ./.venv/lib/python3.13/site-packages (from deepeval) (1.34.0)\n",
      "Requirement already satisfied: portalocker in ./.venv/lib/python3.13/site-packages (from deepeval) (3.1.1)\n",
      "Requirement already satisfied: posthog<4.0.0,>=3.23.0 in ./.venv/lib/python3.13/site-packages (from deepeval) (3.25.0)\n",
      "Requirement already satisfied: pytest in ./.venv/lib/python3.13/site-packages (from deepeval) (8.4.0)\n",
      "Requirement already satisfied: pytest-asyncio in ./.venv/lib/python3.13/site-packages (from deepeval) (1.0.0)\n",
      "Requirement already satisfied: pytest-repeat in ./.venv/lib/python3.13/site-packages (from deepeval) (0.9.4)\n",
      "Requirement already satisfied: pytest-rerunfailures<13.0,>=12.0 in ./.venv/lib/python3.13/site-packages (from deepeval) (12.0)\n",
      "Requirement already satisfied: pytest-xdist in ./.venv/lib/python3.13/site-packages (from deepeval) (3.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in ./.venv/lib/python3.13/site-packages (from deepeval) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.6.0 in ./.venv/lib/python3.13/site-packages (from deepeval) (13.9.4)\n",
      "Requirement already satisfied: sentry-sdk in ./.venv/lib/python3.13/site-packages (from deepeval) (2.29.1)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from deepeval) (80.9.0)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in ./.venv/lib/python3.13/site-packages (from deepeval) (0.9.0)\n",
      "Requirement already satisfied: tenacity<=9.0.0 in ./.venv/lib/python3.13/site-packages (from deepeval) (9.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in ./.venv/lib/python3.13/site-packages (from deepeval) (4.67.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.9 in ./.venv/lib/python3.13/site-packages (from deepeval) (0.16.0)\n",
      "Requirement already satisfied: wheel in ./.venv/lib/python3.13/site-packages (from deepeval) (0.45.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.13/site-packages (from anthropic<0.50.0,>=0.49.0->deepeval) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.13/site-packages (from anthropic<0.50.0,>=0.49.0->deepeval) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from anthropic<0.50.0,>=0.49.0->deepeval) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.13/site-packages (from anthropic<0.50.0,>=0.49.0->deepeval) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.13/site-packages (from anthropic<0.50.0,>=0.49.0->deepeval) (2.11.5)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.13/site-packages (from anthropic<0.50.0,>=0.49.0->deepeval) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in ./.venv/lib/python3.13/site-packages (from anthropic<0.50.0,>=0.49.0->deepeval) (4.13.2)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in ./.venv/lib/python3.13/site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (2.40.3)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in ./.venv/lib/python3.13/site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (15.0.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in ./.venv/lib/python3.13/site-packages (from opentelemetry-api<2.0.0,>=1.24.0->deepeval) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./.venv/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.0 in ./.venv/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.34.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.0 in ./.venv/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.34.0)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.0 in ./.venv/lib/python3.13/site-packages (from opentelemetry-proto==1.34.0->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (5.29.5)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b0 in ./.venv/lib/python3.13/site-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->deepeval) (0.55b0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from posthog<4.0.0,>=3.23.0->deepeval) (1.17.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in ./.venv/lib/python3.13/site-packages (from posthog<4.0.0,>=3.23.0->deepeval) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./.venv/lib/python3.13/site-packages (from posthog<4.0.0,>=3.23.0->deepeval) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in ./.venv/lib/python3.13/site-packages (from posthog<4.0.0,>=3.23.0->deepeval) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging>=17.1 in ./.venv/lib/python3.13/site-packages (from pytest-rerunfailures<13.0,>=12.0->deepeval) (24.2)\n",
      "Requirement already satisfied: iniconfig>=1 in ./.venv/lib/python3.13/site-packages (from pytest->deepeval) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in ./.venv/lib/python3.13/site-packages (from pytest->deepeval) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in ./.venv/lib/python3.13/site-packages (from pytest->deepeval) (2.19.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.31.0->deepeval) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.31.0->deepeval) (2025.4.26)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.13/site-packages (from rich<14.0.0,>=13.6.0->deepeval) (3.0.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.13/site-packages (from typer<1.0.0,>=0.9->deepeval) (1.5.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.13/site-packages (from aiohttp->deepeval) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.13/site-packages (from aiohttp->deepeval) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.13/site-packages (from aiohttp->deepeval) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.13/site-packages (from aiohttp->deepeval) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.13/site-packages (from aiohttp->deepeval) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.13/site-packages (from aiohttp->deepeval) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.13/site-packages (from aiohttp->deepeval) (1.20.0)\n",
      "Requirement already satisfied: execnet>=2.1 in ./.venv/lib/python3.13/site-packages (from pytest-xdist->deepeval) (2.1.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (4.9.1)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->anthropic<0.50.0,>=0.49.0->deepeval) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<0.50.0,>=0.49.0->deepeval) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.13/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.24.0->deepeval) (3.22.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.6.0->deepeval) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic<0.50.0,>=0.49.0->deepeval) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic<0.50.0,>=0.49.0->deepeval) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic<0.50.0,>=0.49.0->deepeval) (0.4.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in ./.venv/lib/python3.13/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.6.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.13/site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.13/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.13/site-packages (from ipywidgets) (9.3.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in ./.venv/lib/python3.13/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./.venv/lib/python3.13/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in ./.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install deepeval\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import GEval, FaithfulnessMetric, ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🎉🥳 Congratulations! You've successfully logged in! 🙌 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "🎉🥳 Congratulations! You've successfully logged in! 🙌 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deepeval\n",
    "deepeval.login_with_confident_api_key(\"NmYNg4nRKJQYoKMBvmXX3JSEQbhrIglazXtFSdrI0kE=\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Correctness\n",
    "\n",
    "Compares the expected output with the actual output to test the response from an LLM.\n",
    "\n",
    "#### Correctness\n",
    "\n",
    "**Definition:** Measures whether the model’s answer is factually correct compared to the expected (ground truth) answer.\n",
    "\n",
    "**How it works:** Compares the actual output to the expected output and checks if the information is accurate and complete.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Expected output: \"Madrid is the capital of Spain.\"\n",
    "* Actual output: \"Madrid.\"\n",
    "* Correctness: Partial, because the answer is not fully complete but factually correct.\n",
    "\n",
    "**Correctness = Is the answer factually right?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95afe3653644523a4af37aa971f3e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11657310100660305\n"
     ]
    }
   ],
   "source": [
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    model=\"gpt-4o\",\n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.EXPECTED_OUTPUT,\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "        evaluation_steps=[\n",
    "        \"Determine whether the actual output is factually correct based on the expected output.\"\n",
    "    ],\n",
    "\n",
    ")\n",
    "\n",
    "gt_answer = \"Madrid is the capital of Spain.\"\n",
    "pred_answer = \"MadriD.\"\n",
    "# Uncomment the line below to test an incorrect prediction\n",
    "#pred_answer = \"Madrid is the capital of France.\"\n",
    "\n",
    "\n",
    "test_case_correctness = LLMTestCase(\n",
    "    input=\"What is the capital of Spain?\",\n",
    "    expected_output=gt_answer,\n",
    "    actual_output=pred_answer,\n",
    ")\n",
    "\n",
    "correctness_metric.measure(test_case_correctness)\n",
    "print(correctness_metric.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test faithfulness\n",
    "\n",
    "#### Faithfulness\n",
    "\n",
    "**Definition:** Measures whether the model’s answer is faithful to the provided context (retrieved documents or facts).\n",
    "**How it works:** Checks if the answer only uses information present in the context and does not hallucinate or invent facts.\n",
    "**Example:**\n",
    "* Context: [\"6\"]\n",
    "* Question: \"What is 3+3?\"\n",
    "* Generated answer: \"6\"\n",
    "* Faithfulness: High, because the answer is directly supported by the context.\n",
    "\n",
    "**Faithfulness = Is the answer strictly based on the provided context?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97a068634674af8ab95185072adfd6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "The score is 1.00 because there are no contradictions between the actual output and the retrieval context, indicating perfect faithfulness.\n"
     ]
    }
   ],
   "source": [
    "faithfulness_metric = FaithfulnessMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4-turbo\",\n",
    "    include_reason=True,\n",
    "    #verbose_mode=True   # More debugging info\n",
    ")\n",
    "\n",
    "# Define test cases for faithfulness metric\n",
    "test_case = LLMTestCase(\n",
    "    input = \"what is 3+3?\",\n",
    "    actual_output=\"6\",\n",
    "    retrieval_context=[\"6\"]\n",
    "\n",
    ")\n",
    "\n",
    "# Test 1: Clear contradiction (should score low)\n",
    "test_contradiction = LLMTestCase(\n",
    "    input=\"What color is the car?\",\n",
    "    actual_output=\"The car is red\",\n",
    "    retrieval_context=[\"The car is blue and parked outside\"]\n",
    ")\n",
    "\n",
    "# Test 2: Supported claim (should score high)\n",
    "test_supported = LLMTestCase(\n",
    "    input=\"What color is the car?\", \n",
    "    actual_output=\"The car is blue\",\n",
    "    retrieval_context=[\"The car is blue and parked outside\"]\n",
    ")\n",
    "\n",
    "# Test 3: Partially supported (should score medium)\n",
    "test_partial = LLMTestCase(\n",
    "    input=\"What do we know about the car?\",\n",
    "    actual_output=\"The car is blue and has leather seats\",  # Only blue is supported\n",
    "    retrieval_context=[\"The car is blue and parked outside\"]\n",
    ")\n",
    "\n",
    "# Test 4: Contextually relevant but not directly answering (should score medium)\n",
    "test_case_with_context = LLMTestCase(\n",
    "    input=\"What number is mentioned?\",\n",
    "    actual_output=\"The number is 7\",  # Wrong number\n",
    "    retrieval_context=[\"The document mentions the number 6\"]  # Contradictory context\n",
    ")\n",
    "\n",
    "\n",
    "faithfulness_metric.measure(test_case)\n",
    "print(faithfulness_metric.score)\n",
    "print(faithfulness_metric.reason)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test contextual relevancy \n",
    "\n",
    "This code evaluates how relevant the retrieved context is to the question and the generated answer using the deepeval library’s ContextualRelevancyMetric.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "- **actual_output:** The answer generated by the model (`\"then go somewhere else.\"`)\n",
    "- **retrieval_context:** A list of context strings retrieved for the question (e.g., `[\"this is a test context\", \"mike is a cat\", \"if the shoes don't fit, then go somewhere else.\"]`)\n",
    "- **gt_answer:** The ground truth answer (`\"if the shoes don't fit, then go somewhere else.\"`)\n",
    "- A `ContextualRelevancyMetric` is created with a threshold and model.\n",
    "- A test case is defined with the question, actual output, context, and expected output.\n",
    "- The metric’s `measure` method evaluates how well the context supports the answer, and prints both the score (between 0 and 1) and the reason for the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa41d4513ec74b5aab7bf9b31df7146f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "The score is 0.33 because the majority of the context, such as 'this is a test context' and 'mike is a cat,' does not relate to the question about shoe fitting. However, the statement 'if the shoes don't fit, then go somewhere else' provides some relevant advice, contributing to the score.\n"
     ]
    }
   ],
   "source": [
    "actual_output = \"then go somewhere else.\"\n",
    "retrieval_context = [\"this is a test context\",\"mike is a cat\",\"if the shoes don't fit, then go somewhere else.\"]\n",
    "gt_answer = \"if the shoes don't fit, then go somewhere else.\"\n",
    "\n",
    "relevance_metric = ContextualRelevancyMetric(\n",
    "    threshold=1,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")\n",
    "relevance_test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output,\n",
    "    retrieval_context=retrieval_context,\n",
    "    expected_output=gt_answer,\n",
    "\n",
    ")\n",
    "\n",
    "relevance_metric.measure(relevance_test_case)\n",
    "print(relevance_metric.score)\n",
    "print(relevance_metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test two different cases together with several metrics together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_case = LLMTestCase(\n",
    "    input=\"What is the capital of Spain?\",\n",
    "    expected_output=\"Madrid is the capital of Spain.\",\n",
    "    actual_output=\"MadriD.\",\n",
    "    retrieval_context=[\"Madrid is the capital of Spain.\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4</span><span style=\"color: #374151; text-decoration-color: #374151\">-turbo, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4\u001b[0m\u001b[38;2;55;65;81m-turbo, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9c2b97088f4469a163249929997425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ❌ Correctness (GEval) (score: 0.1362608186515598, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output is incorrect; 'MadriD.' is not a factual statement about the capital of Spain., error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4-turbo, reason: The score is 1.00 because there are no contradictions between the actual output and the retrieval context, indicating perfect faithfulness., error: None)\n",
      "  - ✅ Contextual Relevancy (score: 1.0, threshold: 1.0, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the statement 'Madrid is the capital of Spain.' directly answers the input question with perfect relevance. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the capital of Spain?\n",
      "  - actual output: MadriD.\n",
      "  - expected output: Madrid is the capital of Spain.\n",
      "  - context: None\n",
      "  - retrieval context: ['Madrid is the capital of Spain.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Correctness (GEval): 50.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "Contextual Relevancy: 50.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Correctness (GEval) (score: 0.5988007074128687, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output is missing the initial part 'if the shoes don't fit,' from the expected output., error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4-turbo, reason: The score is 1.00 because there are no contradictions between the actual output and the retrieval context, indicating perfect faithfulness., error: None)\n",
      "  - ❌ Contextual Relevancy (score: 0.3333333333333333, threshold: 1.0, strict: False, evaluation model: gpt-4o, reason: The score is 0.33 because the majority of the context, such as 'this is a test context' and 'mike is a cat,' does not relate to the question about shoe fitting. However, the statement 'if the shoes don't fit, then go somewhere else' is directly relevant, providing a practical suggestion., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What if these shoes don't fit?\n",
      "  - actual output: then go somewhere else.\n",
      "  - expected output: if the shoes don't fit, then go somewhere else.\n",
      "  - context: None\n",
      "  - retrieval context: ['this is a test context', 'mike is a cat', \"if the shoes don't fit, then go somewhere else.\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Correctness (GEval): 50.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "Contextual Relevancy: 50.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cm12fggwq01qrdamm0wh4o4yy/evaluation/test-runs/cmbjuxlxn0013pb11lprlpsu5/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cm12fggwq01qrdamm0wh4o4yy/evaluation/test-runs/cmbjuxlxn0013pb11lprlpsu5/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cm12fggwq01qrdamm0wh4o4yy/evaluation/test-runs/cmbjuxlxn0013pb11lprlpsu5/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! View results on \n",
       "\u001b]8;id=750963;https://app.confident-ai.com/project/cm12fggwq01qrdamm0wh4o4yy/evaluation/test-runs/cmbjuxlxn0013pb11lprlpsu5/test-cases\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cm12fggwq01qrdamm0wh4o4yy/evaluation/test-runs/cmbjuxlxn0013pb11lprlpsu5/test-\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=750963;https://app.confident-ai.com/project/cm12fggwq01qrdamm0wh4o4yy/evaluation/test-runs/cmbjuxlxn0013pb11lprlpsu5/test-cases\u001b\\\u001b[4;94mcases\u001b[0m\u001b]8;;\u001b\\\u001b[4;94m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_1', success=False, metrics_data=[MetricData(name='Correctness (GEval)', threshold=0.5, success=False, score=0.1362608186515598, reason=\"The actual output is incorrect; 'MadriD.' is not a factual statement about the capital of Spain.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.000915, verbose_logs='Criteria:\\nNone \\n \\nEvaluation Steps:\\n[\\n    \"Determine whether the actual output is factually correct based on the expected output.\"\\n] \\n \\nRubric:\\nNone'), MetricData(name='Faithfulness', threshold=0.7, success=True, score=1.0, reason='The score is 1.00 because there are no contradictions between the actual output and the retrieval context, indicating perfect faithfulness.', strict_mode=False, evaluation_model='gpt-4-turbo', error=None, evaluation_cost=0.00956, verbose_logs='Truths (limit=None):\\n[\\n    \"Madrid is the capital of Spain.\"\\n] \\n \\nClaims:\\n[] \\n \\nVerdicts:\\n[]'), MetricData(name='Contextual Relevancy', threshold=1.0, success=True, score=1.0, reason=\"The score is 1.00 because the statement 'Madrid is the capital of Spain.' directly answers the input question with perfect relevance. Great job!\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0024625000000000003, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"Madrid is the capital of Spain.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    }\\n]')], conversational=False, multimodal=False, input='What is the capital of Spain?', actual_output='MadriD.', expected_output='Madrid is the capital of Spain.', context=None, retrieval_context=['Madrid is the capital of Spain.'], additional_metadata=None), TestResult(name='test_case_0', success=False, metrics_data=[MetricData(name='Correctness (GEval)', threshold=0.5, success=True, score=0.5988007074128687, reason=\"The actual output is missing the initial part 'if the shoes don't fit,' from the expected output.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0009075000000000001, verbose_logs='Criteria:\\nNone \\n \\nEvaluation Steps:\\n[\\n    \"Determine whether the actual output is factually correct based on the expected output.\"\\n] \\n \\nRubric:\\nNone'), MetricData(name='Faithfulness', threshold=0.7, success=True, score=1.0, reason='The score is 1.00 because there are no contradictions between the actual output and the retrieval context, indicating perfect faithfulness.', strict_mode=False, evaluation_model='gpt-4-turbo', error=None, evaluation_cost=0.009640000000000001, verbose_logs='Truths (limit=None):\\n[\\n    \"Mike is a cat.\"\\n] \\n \\nClaims:\\n[] \\n \\nVerdicts:\\n[]'), MetricData(name='Contextual Relevancy', threshold=1.0, success=False, score=0.3333333333333333, reason=\"The score is 0.33 because the majority of the context, such as 'this is a test context' and 'mike is a cat,' does not relate to the question about shoe fitting. However, the statement 'if the shoes don't fit, then go somewhere else' is directly relevant, providing a practical suggestion.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.006262500000000001, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"this is a test context\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'this is a test context\\' does not provide any information related to the question about shoes fitting.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"mike is a cat\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'mike is a cat\\' is unrelated to the question about shoe fitting.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"if the shoes don\\'t fit, then go somewhere else.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    }\\n]')], conversational=False, multimodal=False, input=\"What if these shoes don't fit?\", actual_output='then go somewhere else.', expected_output=\"if the shoes don't fit, then go somewhere else.\", context=None, retrieval_context=['this is a test context', 'mike is a cat', \"if the shoes don't fit, then go somewhere else.\"], additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cm12fggwq01qrdamm0wh4o4yy/evaluation/test-runs/cmbjuxlxn0013pb11lprlpsu5/test-cases')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    test_cases=[relevance_test_case, new_test_case],\n",
    "    metrics=[correctness_metric, faithfulness_metric, relevance_metric]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EvaluationDataset.__init__() got an unexpected keyword argument 'alias'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepeval\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AnswerRelevancyMetric, GEval\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# To run this file: deepeval test run <file_name>.py\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m dataset = \u001b[43mEvaluationDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43malias\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMy dataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;129m@pytest\u001b[39m.mark.parametrize(\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtest_case\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     dataset,\n\u001b[32m     16\u001b[39m )\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_everything\u001b[39m(test_case: LLMTestCase):\n\u001b[32m     18\u001b[39m     test_case = LLMTestCase(\n\u001b[32m     19\u001b[39m         \u001b[38;5;28minput\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mWhat if these shoes don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt fit?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m         \u001b[38;5;66;03m# Replace this with the actual output of your LLM application\u001b[39;00m\n\u001b[32m     21\u001b[39m         actual_output=\u001b[33m\"\u001b[39m\u001b[33mWe offer a 30-day full refund at no extra cost.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m         expected_output=\u001b[33m\"\u001b[39m\u001b[33mYou\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre eligible for a free full refund within 30 days of purchase.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: EvaluationDataset.__init__() got an unexpected keyword argument 'alias'"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "import deepeval\n",
    "from deepeval import assert_test\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import AnswerRelevancyMetric, GEval\n",
    "\n",
    "# To run this file: deepeval test run <file_name>.py\n",
    "\n",
    "dataset = EvaluationDataset(alias=\"My dataset\", test_cases=[])\n",
    "\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "        input=\"What if these shoes don't fit?\",\n",
    "        # Replace this with the actual output of your LLM application\n",
    "        actual_output=\"We offer a 30-day full refund at no extra cost.\",\n",
    "        expected_output=\"You're eligible for a free full refund within 30 days of purchase.\",\n",
    ")\n",
    "\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Correctness - determine if the actual output is correct according to the expected output.\",\n",
    "    evaluation_params=[\n",
    "            LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "            LLMTestCaseParams.EXPECTED_OUTPUT,\n",
    "        ],\n",
    "        strict=True,\n",
    "    \n",
    "\n",
    "\n",
    "@deepeval.log_hyperparameters(model=\"gpt-4\", prompt_template=\"...\")\n",
    "def hyperparameters():\n",
    "    return {\"temperature\": 1, \"chunk size\": 500}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
