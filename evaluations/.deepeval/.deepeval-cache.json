{"test_cases_lookup_map": {"{\"actual_output\": \"MadriD.\", \"context\": null, \"expected_output\": \"Madrid is the capital of Spain.\", \"hyperparameters\": null, \"input\": \"What is the capital of Spain?\", \"retrieval_context\": [\"Madrid is the capital of Spain.\"]}": {"cached_metrics_data": [{"metric_data": {"name": "Correctness (GEval)", "threshold": 0.5, "success": false, "score": 0.1362608186515598, "reason": "The actual output is incorrect; 'MadriD.' is not a factual statement about the capital of Spain.", "strictMode": false, "evaluationModel": "gpt-4o", "evaluationCost": 0, "verboseLogs": "Criteria:\nNone \n \nEvaluation Steps:\n[\n    \"Determine whether the actual output is factually correct based on the expected output.\"\n] \n \nRubric:\nNone"}, "metric_configuration": {"threshold": 0.5, "evaluation_model": "gpt-4o", "strict_mode": false, "include_reason": false, "evaluation_steps": ["Determine whether the actual output is factually correct based on the expected output."], "evaluation_params": ["expected_output", "actual_output"]}}, {"metric_data": {"name": "Faithfulness", "threshold": 0.7, "success": true, "score": 1.0, "reason": "The score is 1.00 because there are no contradictions between the actual output and the retrieval context, indicating perfect faithfulness.", "strictMode": false, "evaluationModel": "gpt-4-turbo", "evaluationCost": 0, "verboseLogs": "Truths (limit=None):\n[\n    \"Madrid is the capital of Spain.\"\n] \n \nClaims:\n[] \n \nVerdicts:\n[]"}, "metric_configuration": {"threshold": 0.7, "evaluation_model": "gpt-4-turbo", "strict_mode": false, "include_reason": true}}, {"metric_data": {"name": "Contextual Relevancy", "threshold": 1.0, "success": true, "score": 1.0, "reason": "The score is 1.00 because the statement 'Madrid is the capital of Spain.' directly answers the input question with perfect relevance. Great job!", "strictMode": false, "evaluationModel": "gpt-4o", "evaluationCost": 0, "verboseLogs": "Verdicts:\n[\n    {\n        \"verdicts\": [\n            {\n                \"statement\": \"Madrid is the capital of Spain.\",\n                \"verdict\": \"yes\",\n                \"reason\": null\n            }\n        ]\n    }\n]"}, "metric_configuration": {"threshold": 1.0, "evaluation_model": "gpt-4o", "strict_mode": false, "include_reason": true}}]}, "{\"actual_output\": \"then go somewhere else.\", \"context\": null, \"expected_output\": \"if the shoes don't fit, then go somewhere else.\", \"hyperparameters\": null, \"input\": \"What if these shoes don't fit?\", \"retrieval_context\": [\"if the shoes don't fit, then go somewhere else.\", \"mike is a cat\", \"this is a test context\"]}": {"cached_metrics_data": [{"metric_data": {"name": "Correctness (GEval)", "threshold": 0.5, "success": true, "score": 0.5988007074128687, "reason": "The actual output is missing the initial part 'if the shoes don't fit,' from the expected output.", "strictMode": false, "evaluationModel": "gpt-4o", "evaluationCost": 0, "verboseLogs": "Criteria:\nNone \n \nEvaluation Steps:\n[\n    \"Determine whether the actual output is factually correct based on the expected output.\"\n] \n \nRubric:\nNone"}, "metric_configuration": {"threshold": 0.5, "evaluation_model": "gpt-4o", "strict_mode": false, "include_reason": false, "evaluation_steps": ["Determine whether the actual output is factually correct based on the expected output."], "evaluation_params": ["expected_output", "actual_output"]}}, {"metric_data": {"name": "Faithfulness", "threshold": 0.7, "success": true, "score": 1.0, "reason": "The score is 1.00 because there are no contradictions between the actual output and the retrieval context, indicating perfect faithfulness.", "strictMode": false, "evaluationModel": "gpt-4-turbo", "evaluationCost": 0, "verboseLogs": "Truths (limit=None):\n[\n    \"Mike is a cat.\"\n] \n \nClaims:\n[] \n \nVerdicts:\n[]"}, "metric_configuration": {"threshold": 0.7, "evaluation_model": "gpt-4-turbo", "strict_mode": false, "include_reason": true}}, {"metric_data": {"name": "Contextual Relevancy", "threshold": 1.0, "success": false, "score": 0.3333333333333333, "reason": "The score is 0.33 because the majority of the context, such as 'this is a test context' and 'mike is a cat,' does not relate to the question about shoe fitting. However, the statement 'if the shoes don't fit, then go somewhere else' is directly relevant, providing a practical suggestion.", "strictMode": false, "evaluationModel": "gpt-4o", "evaluationCost": 0, "verboseLogs": "Verdicts:\n[\n    {\n        \"verdicts\": [\n            {\n                \"statement\": \"this is a test context\",\n                \"verdict\": \"no\",\n                \"reason\": \"The statement 'this is a test context' does not provide any information related to the question about shoes fitting.\"\n            }\n        ]\n    },\n    {\n        \"verdicts\": [\n            {\n                \"statement\": \"mike is a cat\",\n                \"verdict\": \"no\",\n                \"reason\": \"The statement 'mike is a cat' is unrelated to the question about shoe fitting.\"\n            }\n        ]\n    },\n    {\n        \"verdicts\": [\n            {\n                \"statement\": \"if the shoes don't fit, then go somewhere else.\",\n                \"verdict\": \"yes\",\n                \"reason\": null\n            }\n        ]\n    }\n]"}, "metric_configuration": {"threshold": 1.0, "evaluation_model": "gpt-4o", "strict_mode": false, "include_reason": true}}]}}}